{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"Segment Anything","text":"<p>Meta AI Research, FAIR</p> <p>Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick</p> <p>[<code>Paper</code>] [<code>Project</code>] [<code>Demo</code>] [<code>Dataset</code>] [<code>Blog</code>] [<code>BibTeX</code>]</p> <p></p> <p>Segment Anything Model (SAM) \u200b\u4ece\u200b\u8bf8\u5982\u200b\u70b9\u200b\u6216\u200b\u6846\u200b\u4e4b\u7c7b\u200b\u7684\u200b\u8f93\u5165\u200b\u63d0\u793a\u200b\u751f\u6210\u200b\u9ad8\u8d28\u91cf\u200b\u7684\u200b\u5bf9\u8c61\u200b\u63a9\u7801\u200b\uff0c\u200b\u53ef\u200b\u7528\u4e8e\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u5bf9\u8c61\u200b\u751f\u6210\u200b\u63a9\u7801\u200b\u3002\u200b\u5b83\u200b\u5df2\u7ecf\u200b\u5728\u200b\u5305\u542b\u200b1100\u200b\u4e07\u5f20\u200b\u56fe\u50cf\u200b\u548c\u200b11\u200b\u4ebf\u4e2a\u200b\u63a9\u7801\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u5404\u79cd\u200b\u5206\u5272\u200b\u4efb\u52a1\u200b\u4e0a\u200b\u5177\u6709\u200b\u5f3a\u5927\u200b\u7684\u200b\u96f6\u200b\u6837\u672c\u200bzero-shot\u200b\u6027\u80fd\u200b\u3002</p> <p> </p>"},{"location":"#_1","title":"\u5b89\u88c5","text":"<p>\u200b\u4ee3\u7801\u200b\u8981\u6c42\u200b<code>python&gt;=3.8</code>\uff0c\u200b\u4ee5\u53ca\u200b<code>pytorch&gt;=1.7</code>\u200b\u548c\u200b<code>torchvision&gt;=0.8</code>\u3002\u200b\u8bf7\u200b\u6309\u7167\u200b\u8fd9\u91cc\u200b\u7684\u200b\u8bf4\u660e\u200b\u5b89\u88c5\u200bPyTorch\u200b\u548c\u200bTorchVision\u200b\u7684\u200b\u4f9d\u8d56\u200b\u9879\u200b\u3002\u200b\u5f3a\u70c8\u5efa\u8bae\u200b\u5b89\u88c5\u200b\u652f\u6301\u200bCUDA\u200b\u7684\u200bPyTorch\u200b\u548c\u200bTorchVision\u3002</p> <p>\u200b\u514b\u9686\u200b\u5b58\u50a8\u200b\u5e93\u200b\u5e76\u200b\u5728\u200b\u672c\u5730\u200b\u5b89\u88c5\u200b\uff1a</p> <pre><code>git clone https://github.com/facebookresearch/segment-anything.git\ncd segment-anything\npip install -e .\n</code></pre> <p>\u200b\u5efa\u8bae\u200b\u5148\u200bfork\u200b\u5230\u200b\u81ea\u5df1\u200b\u4ed3\u5e93\u200b\u540e\u200b\u518d\u200b\u514b\u9686\u200b</p> <p>\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u5fc5\u8981\u200b\u7684\u200b\u53ef\u9009\u200b\u4f9d\u8d56\u200b\u9879\u200b\uff0c\u200b\u7528\u4e8e\u200b\u63a9\u7801\u200b\u540e\u5904\u7406\u200b\u3001\u200b\u4ee5\u200bCOCO\u200b\u683c\u5f0f\u200b\u4fdd\u5b58\u200b\u63a9\u7801\u200b\u3001\u200b\u793a\u4f8b\u200bjupyter\u200b\u7b14\u8bb0\u672c\u200b\u4ee5\u53ca\u200b\u5c06\u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u4e3a\u200bONNX\u200b\u683c\u5f0f\u200b\u3002\u200b\u8fd0\u884c\u200b\u793a\u4f8b\u200bjupyter\u200b\u7b14\u8bb0\u672c\u200b\u8fd8\u200b\u9700\u8981\u200b<code>jupyter</code>\u3002</p> <pre><code>pip install opencv-python pycocotools matplotlib onnxruntime onnx\n</code></pre>"},{"location":"#_2","title":"\u5165\u95e8","text":"<p>\u200b\u9996\u5148\u200b\u4e0b\u8f7d\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u68c0\u67e5\u70b9\u200b\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u51e0\u884c\u200b\u4ee3\u7801\u200b\u4ece\u200b\u7ed9\u5b9a\u200b\u63d0\u793a\u200b\u4e2d\u200b\u83b7\u53d6\u200b\u63a9\u7801\u200b\uff1a</p> <pre><code>from segment_anything import SamPredictor, sam_model_registry\nsam = sam_model_registry[\"&lt;model_type&gt;\"](checkpoint=\"&lt;path/to/checkpoint&gt;\")\npredictor = SamPredictor(sam)\npredictor.set_image(&lt;your_image&gt;)\nmasks, _, _ = predictor.predict(&lt;input_prompts&gt;)\n</code></pre> <p>\u200b\u6216\u8005\u200b\u4e3a\u200b\u6574\u4e2a\u200b\u56fe\u50cf\u200b\u751f\u6210\u200b\u63a9\u7801\u200b\uff1a</p> <pre><code>from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\nsam = sam_model_registry[\"&lt;model_type&gt;\"](checkpoint=\"&lt;path/to/checkpoint&gt;\")\nmask_generator = SamAutomaticMaskGenerator(sam)\nmasks = mask_generator.generate(&lt;your_image&gt;)\n</code></pre> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u547d\u4ee4\u200b\u884c\u4e3a\u200b\u56fe\u50cf\u200b\u751f\u6210\u200b\u63a9\u7801\u200b\uff1a</p> <pre><code>python scripts/amg.py --checkpoint &lt;path/to/checkpoint&gt; --model-type &lt;model_type&gt; --input &lt;image_or_folder&gt; --output &lt;path/to/output&gt;\n</code></pre> <p>\u200b\u6709\u5173\u200b\u66f4\u200b\u591a\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b\u4f7f\u7528\u200b\u63d0\u793a\u200b\u751f\u6210\u200b\u63a9\u7801\u200b\u548c\u200b\u81ea\u52a8\u200b\u751f\u6210\u200b\u5bf9\u8c61\u200b\u63a9\u7801\u200b\u7684\u200b\u793a\u4f8b\u200b\u7b14\u8bb0\u672c\u200b\u3002</p> <p> </p>"},{"location":"#onnx","title":"ONNX\u200b\u5bfc\u51fa","text":"<p>SAM\u200b\u7684\u200b\u8f7b\u91cf\u7ea7\u200b\u63a9\u7801\u200b\u89e3\u7801\u5668\u200b\u53ef\u4ee5\u200b\u5bfc\u51fa\u200b\u4e3a\u200bONNX\u200b\u683c\u5f0f\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u652f\u6301\u200bONNX\u200b\u8fd0\u884c\u200b\u65f6\u200b\u7684\u200b\u4efb\u4f55\u200b\u73af\u5883\u4e2d\u8fd0\u884c\u200b\uff0c\u200b\u4f8b\u5982\u200b\u5728\u200b\u6f14\u793a\u200b\u4e2d\u200b\u5c55\u793a\u200b\u7684\u200b\u6d4f\u89c8\u5668\u200b\u4e2d\u200b\u3002\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u5bfc\u51fa\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>python scripts/export_onnx_model.py --checkpoint &lt;path/to/checkpoint&gt; --model-type &lt;model_type&gt; --output &lt;path/to/output&gt;\n</code></pre> <p>\u200b\u8bf7\u53c2\u9605\u200b\u793a\u4f8b\u200b\u7b14\u8bb0\u672c\u200b\u4ee5\u200b\u4e86\u89e3\u200b\u5982\u4f55\u200b\u901a\u8fc7\u200bSAM\u200b\u7684\u200b\u9aa8\u5e72\u200b\u8fdb\u884c\u200b\u56fe\u50cf\u200b\u9884\u5904\u7406\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200bONNX\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u63a9\u7801\u200b\u9884\u6d4b\u200b\u7684\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u3002\u200b\u5efa\u8bae\u200b\u4f7f\u7528\u200bPyTorch\u200b\u7684\u200b\u6700\u65b0\u200b\u7a33\u5b9a\u200b\u7248\u672c\u200b\u8fdb\u884c\u200bONNX\u200b\u5bfc\u51fa\u200b\u3002</p>"},{"location":"#web","title":"Web\u200b\u6f14\u793a","text":"<p><code>demo/</code>\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u6709\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u5355\u9875\u200bReact\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\uff0c\u200b\u5c55\u793a\u200b\u4e86\u200b\u5982\u4f55\u200b\u5728\u200b\u652f\u6301\u200b\u591a\u7ebf\u7a0b\u200b\u7684\u200bWeb\u200b\u6d4f\u89c8\u5668\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u5bfc\u51fa\u200b\u7684\u200bONNX\u200b\u6a21\u578b\u200b\u8fd0\u884c\u200b\u63a9\u7801\u200b\u9884\u6d4b\u200b\u3002\u200b\u8bf7\u200b\u67e5\u770b\u200b<code>demo/README.md</code>\u200b\u4ee5\u200b\u83b7\u53d6\u200b\u66f4\u200b\u591a\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u3002</p>"},{"location":"#_3","title":"\u6a21\u578b\u200b\u68c0\u67e5\u70b9","text":"<p>\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e09\u4e2a\u200b\u6a21\u578b\u200b\u7248\u672c\u200b\uff0c\u200b\u5177\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u9aa8\u5e72\u200b\u5927\u5c0f\u200b\u3002\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8fd0\u884c\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>from segment_anything import sam_model_registry\nsam = sam_model_registry[\"&lt;model_type&gt;\"](checkpoint=\"&lt;path/to/checkpoint&gt;\")\n</code></pre> <p>\u200b\u5355\u51fb\u200b\u4e0b\u9762\u200b\u7684\u200b\u94fe\u63a5\u200b\u4e0b\u8f7d\u200b\u76f8\u5e94\u200b\u6a21\u578b\u200b\u7c7b\u578b\u200b\u7684\u200b\u68c0\u67e5\u70b9\u200b\u3002</p> <ul> <li><code>default</code>\u200b\u6216\u200b<code>vit_h</code>\uff1aViT-H SAM\u200b\u6a21\u578b\u200b\u3002</li> <li><code>vit_l</code>\uff1aViT-L SAM\u200b\u6a21\u578b\u200b\u3002</li> <li><code>vit_b</code>\uff1aViT-B SAM\u200b\u6a21\u578b\u200b\u3002</li> </ul> <p>b:base\u200b\u57fa\u7840\u200b\u6a21\u578b\u200b</p> <p>l:large\u200b\u8f83\u5927\u200b\u6a21\u578b\u200b</p> <p>h:huge\u200b\u6700\u5927\u200b\u7684\u200b\u6a21\u578b\u200b</p>"},{"location":"#_4","title":"\u6570\u636e\u200b\u96c6","text":"<p>\u200b\u8bf7\u53c2\u9605\u200b\u6b64\u5904\u200b\u4ee5\u200b\u83b7\u53d6\u200b\u6709\u5173\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6982\u8ff0\u200b\u3002\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6b64\u5904\u200b\u4e0b\u8f7d\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u901a\u8fc7\u200b\u4e0b\u8f7d\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u60a8\u200b\u540c\u610f\u200b\u5df2\u200b\u9605\u8bfb\u200b\u5e76\u200b\u63a5\u53d7\u200b\u4e86\u200bSA-1B\u200b\u6570\u636e\u200b\u96c6\u200b\u7814\u7a76\u200b\u8bb8\u53ef\u200b\u6761\u6b3e\u200b\u3002</p> <p>\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u7684\u200b\u63a9\u7801\u200b\u4fdd\u5b58\u200b\u4e3a\u200bjson\u200b\u6587\u4ef6\u200b\u3002\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4ee5\u4e0b\u200b\u683c\u5f0f\u200b\u7684\u200bPython\u200b\u5b57\u5178\u200b\u4e2d\u200b\u52a0\u8f7d\u200b\u3002</p> <pre><code>{\n    \"image\"                 : image_info,\n    \"annotations\"           : [annotation],\n}\n\nimage_info {\n    \"image_id\"              : int,              # \u200b\u56fe\u50cf\u200bid\n    \"width\"                 : int,              # \u200b\u56fe\u50cf\u200b\u5bbd\u5ea6\u200b\n    \"height\"                : int,              # \u200b\u56fe\u50cf\u200b\u9ad8\u5ea6\u200b\n    \"file_name\"             : str,              # \u200b\u56fe\u50cf\u200b\u6587\u4ef6\u540d\u200b\n}\n\nannotation {\n    \"id\"                    : int,              # \u200b\u6ce8\u91ca\u200bid\n    \"segmentation\"          : dict,             # \u200b\u4ee5\u200bCOCO RLE\u200b\u683c\u5f0f\u200b\u4fdd\u5b58\u200b\u7684\u200b\u63a9\u7801\u200b\u3002\n    \"bbox\"                  : [x, y, w, h],     # \u200b\u63a9\u7801\u200b\u5468\u56f4\u200b\u7684\u200b\u6846\u200b\uff0c\u200b\u4ee5\u200bXYWH\u200b\u683c\u5f0f\u200b\u8868\u793a\u200b\n    \"area\"                  : int,              # \u200b\u63a9\u7801\u200b\u7684\u200b\u50cf\u7d20\u200b\u9762\u79ef\u200b\n    \"predicted_iou\"         : float,            # \u200b\u6a21\u578b\u200b\u5bf9\u200b\u63a9\u7801\u200b\u8d28\u91cf\u200b\u7684\u200b\u81ea\u8eab\u200b\u9884\u6d4b\u200b\n    \"stability_score\"       : float,            # \u200b\u63a9\u7801\u200b\u8d28\u91cf\u200b\u7684\u200b\u5ea6\u91cf\u200b\n    \"crop_box\"              : [x, y, w, h],     # \u200b\u7528\u4e8e\u200b\u751f\u6210\u200b\u63a9\u7801\u200b\u7684\u200b\u56fe\u50cf\u200b\u7684\u200b\u88c1\u526a\u200b\uff0c\u200b\u4ee5\u200bXYWH\u200b\u683c\u5f0f\u200b\u8868\u793a\u200b\n    \"point_coords\"          : [[x, y]],         # \u200b\u8f93\u5165\u200b\u6a21\u578b\u200b\u751f\u6210\u200b\u63a9\u7801\u200b\u7684\u200b\u70b9\u200b\u5750\u6807\u200b\n}\n</code></pre> <p>\u200b\u56fe\u50cf\u200bID\u200b\u53ef\u4ee5\u200b\u5728\u200b<code>sa_images_ids.txt</code>\u200b\u4e2d\u200b\u627e\u5230\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0a\u8ff0\u200b\u94fe\u63a5\u200b\u4e0b\u8f7d\u200b\u3002</p> <p>\u200b\u8981\u200b\u5c06\u200bCOCO RLE\u200b\u683c\u5f0f\u200b\u7684\u200b\u63a9\u7801\u200b\u89e3\u7801\u200b\u4e3a\u200b\u4e8c\u8fdb\u5236\u200b\uff1a</p> <pre><code>from pycocotools import mask as mask_utils\nmask = mask_utils.decode(annotation[\"segmentation\"])\n</code></pre> <p>\u200b\u8bf7\u53c2\u9605\u200b\u6b64\u5904\u200b\u4ee5\u200b\u83b7\u53d6\u200b\u6709\u5173\u200b\u5982\u4f55\u200b\u64cd\u4f5c\u200b\u4ee5\u200bRLE\u200b\u683c\u5f0f\u200b\u5b58\u50a8\u200b\u7684\u200b\u63a9\u7801\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u8bf4\u660e\u200b\u3002</p>"},{"location":"#segment-anything_1","title":"\u5f15\u7528\u200bSegment Anything","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5728\u200b\u7814\u7a76\u200b\u4e2d\u200b\u4f7f\u7528\u200bSAM\u200b\u6216\u200bSA-1B\uff0c\u200b\u8bf7\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200bBibTeX\u200b\u6761\u76ee\u200b\u3002</p> <pre><code>@article{kirillov2023segany,\n  title={Segment Anything},\n  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\\'a}r, Piotr and Girshick, Ross},\n  journal={arXiv:2304.02643},\n  year={2023}\n}\n</code></pre>"},{"location":"notebooks/automatic_mask_generator_example/","title":"\u81ea\u52a8\u200b\u751f\u6210\u200b\u5bf9\u8c61\u200b\u63a9\u7801","text":"In\u00a0[1]: Copied! <pre># Copyright (c) Meta Platforms, Inc. and affiliates.\n</pre> # Copyright (c) Meta Platforms, Inc. and affiliates. <p>Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image. This method was used to generate the dataset SA-1B.</p> <p>The class <code>SamAutomaticMaskGenerator</code> implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes.</p> In\u00a0[2]: Copied! <pre>from IPython.display import display, HTML\ndisplay(HTML(\n\"\"\"\n&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\"&gt;\n  &lt;img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/&gt;\n&lt;/a&gt;\n\"\"\"\n))\n</pre> from IPython.display import display, HTML display(HTML( \"\"\"   \"\"\" )) <p>If running locally using jupyter, first install <code>segment_anything</code> in your environment using the installation instructions in the repository. If running from Google Colab, set <code>using_colab=True</code> below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'-&gt;'Notebook Settings'-&gt;'Hardware accelerator'.</p> In\u00a0[3]: Copied! <pre>using_colab = False\n</pre> using_colab = False In\u00a0[4]: Copied! <pre>if using_colab:\n    import torch\n    import torchvision\n    print(\"PyTorch version:\", torch.__version__)\n    print(\"Torchvision version:\", torchvision.__version__)\n    print(\"CUDA is available:\", torch.cuda.is_available())\n    import sys\n    !{sys.executable} -m pip install opencv-python matplotlib\n    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n    \n    !mkdir images\n    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n        \n    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n</pre> if using_colab:     import torch     import torchvision     print(\"PyTorch version:\", torch.__version__)     print(\"Torchvision version:\", torchvision.__version__)     print(\"CUDA is available:\", torch.cuda.is_available())     import sys     !{sys.executable} -m pip install opencv-python matplotlib     !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'          !mkdir images     !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg              !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth In\u00a0[5]: Copied! <pre>import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2\n</pre> import numpy as np import torch import matplotlib.pyplot as plt import cv2 In\u00a0[6]: Copied! <pre>def show_anns(anns):\n    if len(anns) == 0:\n        return\n    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n    img[:,:,3] = 0\n    for ann in sorted_anns:\n        m = ann['segmentation']\n        color_mask = np.concatenate([np.random.random(3), [0.35]])\n        img[m] = color_mask\n    ax.imshow(img)\n</pre> def show_anns(anns):     if len(anns) == 0:         return     sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)     ax = plt.gca()     ax.set_autoscale_on(False)      img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))     img[:,:,3] = 0     for ann in sorted_anns:         m = ann['segmentation']         color_mask = np.concatenate([np.random.random(3), [0.35]])         img[m] = color_mask     ax.imshow(img) In\u00a0[7]: Copied! <pre>image = cv2.imread('images/dog.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</pre> image = cv2.imread('images/dog.jpg') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) In\u00a0[8]: Copied! <pre>plt.figure(figsize=(20,20))\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(20,20)) plt.imshow(image) plt.axis('off') plt.show() <p>To run automatic mask generation, provide a SAM model to the <code>SamAutomaticMaskGenerator</code> class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended.</p> In\u00a0[10]: Copied! <pre>import sys\nsys.path.append(\"..\")\nfrom segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\nsam_checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\n\ndevice = \"cuda\"\n\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\n\nmask_generator = SamAutomaticMaskGenerator(sam)\n</pre> import sys sys.path.append(\"..\") from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor  sam_checkpoint = \"sam_vit_h_4b8939.pth\" model_type = \"vit_h\"  device = \"cuda\"  sam = sam_model_registry[model_type](checkpoint=sam_checkpoint) sam.to(device=device)  mask_generator = SamAutomaticMaskGenerator(sam) <p>To generate masks, just run <code>generate</code> on an image.</p> In\u00a0[11]: Copied! <pre>masks = mask_generator.generate(image)\n</pre> masks = mask_generator.generate(image) <p>Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:</p> <ul> <li><code>segmentation</code> : the mask</li> <li><code>area</code> : the area of the mask in pixels</li> <li><code>bbox</code> : the boundary box of the mask in XYWH format</li> <li><code>predicted_iou</code> : the model's own prediction for the quality of the mask</li> <li><code>point_coords</code> : the sampled input point that generated this mask</li> <li><code>stability_score</code> : an additional measure of mask quality</li> <li><code>crop_box</code> : the crop of the image used to generate this mask in XYWH format</li> </ul> In\u00a0[13]: Copied! <pre>print(len(masks))\nprint(masks[0].keys())\n</pre> print(len(masks)) print(masks[0].keys()) <pre>dict_keys(['segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'])\n</pre> <p>Show all the masks overlayed on the image.</p> In\u00a0[28]: Copied! <pre>plt.figure(figsize=(20,20))\nplt.imshow(image)\nshow_anns(masks)\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(20,20)) plt.imshow(image) show_anns(masks) plt.axis('off') plt.show()  <p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p> In\u00a0[24]: Copied! <pre>mask_generator_2 = SamAutomaticMaskGenerator(\n    model=sam,\n    points_per_side=32,\n    pred_iou_thresh=0.86,\n    stability_score_thresh=0.92,\n    crop_n_layers=1,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=100,  # Requires open-cv to run post-processing\n)\n</pre> mask_generator_2 = SamAutomaticMaskGenerator(     model=sam,     points_per_side=32,     pred_iou_thresh=0.86,     stability_score_thresh=0.92,     crop_n_layers=1,     crop_n_points_downscale_factor=2,     min_mask_region_area=100,  # Requires open-cv to run post-processing ) In\u00a0[25]: Copied! <pre>masks2 = mask_generator_2.generate(image)\n</pre> masks2 = mask_generator_2.generate(image) In\u00a0[26]: Copied! <pre>len(masks2)\n</pre> len(masks2) Out[26]: <pre>90</pre> In\u00a0[27]: Copied! <pre>plt.figure(figsize=(20,20))\nplt.imshow(image)\nshow_anns(masks2)\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(20,20)) plt.imshow(image) show_anns(masks2) plt.axis('off') plt.show()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/automatic_mask_generator_example/#automatically-generating-object-masks-with-sam","title":"Automatically generating object masks with SAM\u00b6","text":""},{"location":"notebooks/automatic_mask_generator_example/#environment-set-up","title":"Environment Set-up\u00b6","text":""},{"location":"notebooks/automatic_mask_generator_example/#set-up","title":"Set-up\u00b6","text":""},{"location":"notebooks/automatic_mask_generator_example/#example-image","title":"Example image\u00b6","text":""},{"location":"notebooks/automatic_mask_generator_example/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"notebooks/automatic_mask_generator_example/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":""},{"location":"notebooks/onnx_model_example/","title":"\u4f7f\u7528\u200bONNX\u200b\u6a21\u578b\u200b\u4ece\u200b\u63d0\u793a\u200b\u751f\u6210\u200b\u63a9\u7801","text":"In\u00a0[\u00a0]: Copied! <pre># Copyright (c) Meta Platforms, Inc. and affiliates.\n</pre> # Copyright (c) Meta Platforms, Inc. and affiliates. <p>SAM's prompt encoder and mask decoder are very lightweight, which allows for efficient computation of a mask given user input. This notebook shows an example of how to export and use this lightweight component of the model in ONNX format, allowing it to run on a variety of platforms that support an ONNX runtime.</p> In\u00a0[4]: Copied! <pre>from IPython.display import display, HTML\ndisplay(HTML(\n\"\"\"\n&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb\"&gt;\n  &lt;img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/&gt;\n&lt;/a&gt;\n\"\"\"\n))\n</pre> from IPython.display import display, HTML display(HTML( \"\"\"   \"\"\" )) <p>If running locally using jupyter, first install <code>segment_anything</code> in your environment using the installation instructions in the repository. The latest stable versions of PyTorch and ONNX are recommended for this notebook. If running from Google Colab, set <code>using_colab=True</code> below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'-&gt;'Notebook Settings'-&gt;'Hardware accelerator'.</p> In\u00a0[5]: Copied! <pre>using_colab = False\n</pre> using_colab = False In\u00a0[6]: Copied! <pre>if using_colab:\n    import torch\n    import torchvision\n    print(\"PyTorch version:\", torch.__version__)\n    print(\"Torchvision version:\", torchvision.__version__)\n    print(\"CUDA is available:\", torch.cuda.is_available())\n    import sys\n    !{sys.executable} -m pip install opencv-python matplotlib onnx onnxruntime\n    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n    \n    !mkdir images\n    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n        \n    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n</pre> if using_colab:     import torch     import torchvision     print(\"PyTorch version:\", torch.__version__)     print(\"Torchvision version:\", torchvision.__version__)     print(\"CUDA is available:\", torch.cuda.is_available())     import sys     !{sys.executable} -m pip install opencv-python matplotlib onnx onnxruntime     !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'          !mkdir images     !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg              !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth <p>Note that this notebook requires both the <code>onnx</code> and <code>onnxruntime</code> optional dependencies, in addition to <code>opencv-python</code> and <code>matplotlib</code> for visualization.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom segment_anything import sam_model_registry, SamPredictor\nfrom segment_anything.utils.onnx import SamOnnxModel\n\nimport onnxruntime\nfrom onnxruntime.quantization import QuantType\nfrom onnxruntime.quantization.quantize import quantize_dynamic\n</pre> import torch import numpy as np import cv2 import matplotlib.pyplot as plt from segment_anything import sam_model_registry, SamPredictor from segment_anything.utils.onnx import SamOnnxModel  import onnxruntime from onnxruntime.quantization import QuantType from onnxruntime.quantization.quantize import quantize_dynamic In\u00a0[\u00a0]: Copied! <pre>def show_mask(mask, ax):\n    color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n    \ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n    \ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n</pre> def show_mask(mask, ax):     color = np.array([30/255, 144/255, 255/255, 0.6])     h, w = mask.shape[-2:]     mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)     ax.imshow(mask_image)      def show_points(coords, labels, ax, marker_size=375):     pos_points = coords[labels==1]     neg_points = coords[labels==0]     ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)     ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)         def show_box(box, ax):     x0, y0 = box[0], box[1]     w, h = box[2] - box[0], box[3] - box[1]     ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    <p>Set the path below to a SAM model checkpoint, then load the model. This will be needed to both export the model and to calculate embeddings for the model.</p> In\u00a0[\u00a0]: Copied! <pre>checkpoint = \"sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\n</pre> checkpoint = \"sam_vit_h_4b8939.pth\" model_type = \"vit_h\" In\u00a0[\u00a0]: Copied! <pre>sam = sam_model_registry[model_type](checkpoint=checkpoint)\n</pre> sam = sam_model_registry[model_type](checkpoint=checkpoint) <p>The script <code>segment-anything/scripts/export_onnx_model.py</code> can be used to export the necessary portion of SAM. Alternatively, run the following code to export an ONNX model. If you have already exported a model, set the path below and skip to the next section. Assure that the exported ONNX model aligns with the checkpoint and model type set above. This notebook expects the model was exported with the parameter <code>return_single_mask=True</code>.</p> In\u00a0[\u00a0]: Copied! <pre>onnx_model_path = None  # Set to use an already exported model, then skip to the next section.\n</pre> onnx_model_path = None  # Set to use an already exported model, then skip to the next section. In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nonnx_model_path = \"sam_onnx_example.onnx\"\n\nonnx_model = SamOnnxModel(sam, return_single_mask=True)\n\ndynamic_axes = {\n    \"point_coords\": {1: \"num_points\"},\n    \"point_labels\": {1: \"num_points\"},\n}\n\nembed_dim = sam.prompt_encoder.embed_dim\nembed_size = sam.prompt_encoder.image_embedding_size\nmask_input_size = [4 * x for x in embed_size]\ndummy_inputs = {\n    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n}\noutput_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    with open(onnx_model_path, \"wb\") as f:\n        torch.onnx.export(\n            onnx_model,\n            tuple(dummy_inputs.values()),\n            f,\n            export_params=True,\n            verbose=False,\n            opset_version=17,\n            do_constant_folding=True,\n            input_names=list(dummy_inputs.keys()),\n            output_names=output_names,\n            dynamic_axes=dynamic_axes,\n        )\n</pre> import warnings  onnx_model_path = \"sam_onnx_example.onnx\"  onnx_model = SamOnnxModel(sam, return_single_mask=True)  dynamic_axes = {     \"point_coords\": {1: \"num_points\"},     \"point_labels\": {1: \"num_points\"}, }  embed_dim = sam.prompt_encoder.embed_dim embed_size = sam.prompt_encoder.image_embedding_size mask_input_size = [4 * x for x in embed_size] dummy_inputs = {     \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),     \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),     \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),     \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),     \"has_mask_input\": torch.tensor([1], dtype=torch.float),     \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float), } output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]  with warnings.catch_warnings():     warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)     warnings.filterwarnings(\"ignore\", category=UserWarning)     with open(onnx_model_path, \"wb\") as f:         torch.onnx.export(             onnx_model,             tuple(dummy_inputs.values()),             f,             export_params=True,             verbose=False,             opset_version=17,             do_constant_folding=True,             input_names=list(dummy_inputs.keys()),             output_names=output_names,             dynamic_axes=dynamic_axes,         )     <p>If desired, the model can additionally be quantized and optimized. We find this improves web runtime significantly for negligible change in qualitative performance. Run the next cell to quantize the model, or skip to the next section otherwise.</p> In\u00a0[\u00a0]: Copied! <pre>onnx_model_quantized_path = \"sam_onnx_quantized_example.onnx\"\nquantize_dynamic(\n    model_input=onnx_model_path,\n    model_output=onnx_model_quantized_path,\n    optimize_model=True,\n    per_channel=False,\n    reduce_range=False,\n    weight_type=QuantType.QUInt8,\n)\nonnx_model_path = onnx_model_quantized_path\n</pre> onnx_model_quantized_path = \"sam_onnx_quantized_example.onnx\" quantize_dynamic(     model_input=onnx_model_path,     model_output=onnx_model_quantized_path,     optimize_model=True,     per_channel=False,     reduce_range=False,     weight_type=QuantType.QUInt8, ) onnx_model_path = onnx_model_quantized_path In\u00a0[\u00a0]: Copied! <pre>image = cv2.imread('images/truck.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</pre> image = cv2.imread('images/truck.jpg') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10,10))\nplt.imshow(image)\nplt.axis('on')\nplt.show()\n</pre> plt.figure(figsize=(10,10)) plt.imshow(image) plt.axis('on') plt.show() <p>Here as an example, we use <code>onnxruntime</code> in python on CPU to execute the ONNX model. However, any platform that supports an ONNX runtime could be used in principle. Launch the runtime session below:</p> In\u00a0[\u00a0]: Copied! <pre>ort_session = onnxruntime.InferenceSession(onnx_model_path)\n</pre> ort_session = onnxruntime.InferenceSession(onnx_model_path) <p>To use the ONNX model, the image must first be pre-processed using the SAM image encoder. This is a heavier weight process best performed on GPU. SamPredictor can be used as normal, then <code>.get_image_embedding()</code> will retreive the intermediate features.</p> In\u00a0[\u00a0]: Copied! <pre>sam.to(device='cuda')\npredictor = SamPredictor(sam)\n</pre> sam.to(device='cuda') predictor = SamPredictor(sam) In\u00a0[\u00a0]: Copied! <pre>predictor.set_image(image)\n</pre> predictor.set_image(image) In\u00a0[\u00a0]: Copied! <pre>image_embedding = predictor.get_image_embedding().cpu().numpy()\n</pre> image_embedding = predictor.get_image_embedding().cpu().numpy() In\u00a0[\u00a0]: Copied! <pre>image_embedding.shape\n</pre> image_embedding.shape <p>The ONNX model has a different input signature than <code>SamPredictor.predict</code>. The following inputs must all be supplied. Note the special cases for both point and mask inputs. All inputs are <code>np.float32</code>.</p> <ul> <li><code>image_embeddings</code>: The image embedding from <code>predictor.get_image_embedding()</code>. Has a batch index of length 1.</li> <li><code>point_coords</code>: Coordinates of sparse input prompts, corresponding to both point inputs and box inputs. Boxes are encoded using two points, one for the top-left corner and one for the bottom-right corner. Coordinates must already be transformed to long-side 1024. Has a batch index of length 1.</li> <li><code>point_labels</code>: Labels for the sparse input prompts. 0 is a negative input point, 1 is a positive input point, 2 is a top-left box corner, 3 is a bottom-right box corner, and -1 is a padding point. If there is no box input, a single padding point with label -1 and coordinates (0.0, 0.0) should be concatenated.</li> <li><code>mask_input</code>: A mask input to the model with shape 1x1x256x256. This must be supplied even if there is no mask input. In this case, it can just be zeros.</li> <li><code>has_mask_input</code>: An indicator for the mask input. 1 indicates a mask input, 0 indicates no mask input.</li> <li><code>orig_im_size</code>: The size of the input image in (H,W) format, before any transformation.</li> </ul> <p>Additionally, the ONNX model does not threshold the output mask logits. To obtain a binary mask, threshold at <code>sam.mask_threshold</code> (equal to 0.0).</p> In\u00a0[\u00a0]: Copied! <pre>input_point = np.array([[500, 375]])\ninput_label = np.array([1])\n</pre> input_point = np.array([[500, 375]]) input_label = np.array([1]) <p>Add a batch index, concatenate a padding point, and transform.</p> In\u00a0[\u00a0]: Copied! <pre>onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\nonnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)\n\nonnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)\n</pre> onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :] onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)  onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)  <p>Create an empty mask input and an indicator for no mask.</p> In\u00a0[\u00a0]: Copied! <pre>onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\nonnx_has_mask_input = np.zeros(1, dtype=np.float32)\n</pre> onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32) onnx_has_mask_input = np.zeros(1, dtype=np.float32) <p>Package the inputs to run in the onnx model</p> In\u00a0[\u00a0]: Copied! <pre>ort_inputs = {\n    \"image_embeddings\": image_embedding,\n    \"point_coords\": onnx_coord,\n    \"point_labels\": onnx_label,\n    \"mask_input\": onnx_mask_input,\n    \"has_mask_input\": onnx_has_mask_input,\n    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n}\n</pre> ort_inputs = {     \"image_embeddings\": image_embedding,     \"point_coords\": onnx_coord,     \"point_labels\": onnx_label,     \"mask_input\": onnx_mask_input,     \"has_mask_input\": onnx_has_mask_input,     \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32) } <p>Predict a mask and threshold it.</p> In\u00a0[\u00a0]: Copied! <pre>masks, _, low_res_logits = ort_session.run(None, ort_inputs)\nmasks = masks &gt; predictor.model.mask_threshold\n</pre> masks, _, low_res_logits = ort_session.run(None, ort_inputs) masks = masks &gt; predictor.model.mask_threshold In\u00a0[\u00a0]: Copied! <pre>masks.shape\n</pre> masks.shape In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_mask(masks, plt.gca())\nshow_points(input_point, input_label, plt.gca())\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(10,10)) plt.imshow(image) show_mask(masks, plt.gca()) show_points(input_point, input_label, plt.gca()) plt.axis('off') plt.show()  In\u00a0[\u00a0]: Copied! <pre>input_point = np.array([[500, 375], [1125, 625]])\ninput_label = np.array([1, 1])\n\n# Use the mask output from the previous run. It is already in the correct form for input to the ONNX model.\nonnx_mask_input = low_res_logits\n</pre> input_point = np.array([[500, 375], [1125, 625]]) input_label = np.array([1, 1])  # Use the mask output from the previous run. It is already in the correct form for input to the ONNX model. onnx_mask_input = low_res_logits <p>Transform the points as in the previous example.</p> In\u00a0[\u00a0]: Copied! <pre>onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\nonnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)\n\nonnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)\n</pre> onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :] onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)  onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32) <p>The <code>has_mask_input</code> indicator is now 1.</p> In\u00a0[\u00a0]: Copied! <pre>onnx_has_mask_input = np.ones(1, dtype=np.float32)\n</pre> onnx_has_mask_input = np.ones(1, dtype=np.float32) <p>Package inputs, then predict and threshold the mask.</p> In\u00a0[\u00a0]: Copied! <pre>ort_inputs = {\n    \"image_embeddings\": image_embedding,\n    \"point_coords\": onnx_coord,\n    \"point_labels\": onnx_label,\n    \"mask_input\": onnx_mask_input,\n    \"has_mask_input\": onnx_has_mask_input,\n    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n}\n\nmasks, _, _ = ort_session.run(None, ort_inputs)\nmasks = masks &gt; predictor.model.mask_threshold\n</pre> ort_inputs = {     \"image_embeddings\": image_embedding,     \"point_coords\": onnx_coord,     \"point_labels\": onnx_label,     \"mask_input\": onnx_mask_input,     \"has_mask_input\": onnx_has_mask_input,     \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32) }  masks, _, _ = ort_session.run(None, ort_inputs) masks = masks &gt; predictor.model.mask_threshold In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_mask(masks, plt.gca())\nshow_points(input_point, input_label, plt.gca())\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(10,10)) plt.imshow(image) show_mask(masks, plt.gca()) show_points(input_point, input_label, plt.gca()) plt.axis('off') plt.show()  In\u00a0[\u00a0]: Copied! <pre>input_box = np.array([425, 600, 700, 875])\ninput_point = np.array([[575, 750]])\ninput_label = np.array([0])\n</pre> input_box = np.array([425, 600, 700, 875]) input_point = np.array([[575, 750]]) input_label = np.array([0]) <p>Add a batch index, concatenate a box and point inputs, add the appropriate labels for the box corners, and transform. There is no padding point since the input includes a box input.</p> In\u00a0[\u00a0]: Copied! <pre>onnx_box_coords = input_box.reshape(2, 2)\nonnx_box_labels = np.array([2,3])\n\nonnx_coord = np.concatenate([input_point, onnx_box_coords], axis=0)[None, :, :]\nonnx_label = np.concatenate([input_label, onnx_box_labels], axis=0)[None, :].astype(np.float32)\n\nonnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32)\n</pre> onnx_box_coords = input_box.reshape(2, 2) onnx_box_labels = np.array([2,3])  onnx_coord = np.concatenate([input_point, onnx_box_coords], axis=0)[None, :, :] onnx_label = np.concatenate([input_label, onnx_box_labels], axis=0)[None, :].astype(np.float32)  onnx_coord = predictor.transform.apply_coords(onnx_coord, image.shape[:2]).astype(np.float32) <p>Package inputs, then predict and threshold the mask.</p> In\u00a0[\u00a0]: Copied! <pre>onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\nonnx_has_mask_input = np.zeros(1, dtype=np.float32)\n\nort_inputs = {\n    \"image_embeddings\": image_embedding,\n    \"point_coords\": onnx_coord,\n    \"point_labels\": onnx_label,\n    \"mask_input\": onnx_mask_input,\n    \"has_mask_input\": onnx_has_mask_input,\n    \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32)\n}\n\nmasks, _, _ = ort_session.run(None, ort_inputs)\nmasks = masks &gt; predictor.model.mask_threshold\n</pre> onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32) onnx_has_mask_input = np.zeros(1, dtype=np.float32)  ort_inputs = {     \"image_embeddings\": image_embedding,     \"point_coords\": onnx_coord,     \"point_labels\": onnx_label,     \"mask_input\": onnx_mask_input,     \"has_mask_input\": onnx_has_mask_input,     \"orig_im_size\": np.array(image.shape[:2], dtype=np.float32) }  masks, _, _ = ort_session.run(None, ort_inputs) masks = masks &gt; predictor.model.mask_threshold In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 10))\nplt.imshow(image)\nshow_mask(masks[0], plt.gca())\nshow_box(input_box, plt.gca())\nshow_points(input_point, input_label, plt.gca())\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(10, 10)) plt.imshow(image) show_mask(masks[0], plt.gca()) show_box(input_box, plt.gca()) show_points(input_point, input_label, plt.gca()) plt.axis('off') plt.show()"},{"location":"notebooks/onnx_model_example/#produces-masks-from-prompts-using-an-onnx-model","title":"Produces masks from prompts using an ONNX model\u00b6","text":""},{"location":"notebooks/onnx_model_example/#environment-set-up","title":"Environment Set-up\u00b6","text":""},{"location":"notebooks/onnx_model_example/#set-up","title":"Set-up\u00b6","text":""},{"location":"notebooks/onnx_model_example/#export-an-onnx-model","title":"Export an ONNX model\u00b6","text":""},{"location":"notebooks/onnx_model_example/#example-image","title":"Example Image\u00b6","text":""},{"location":"notebooks/onnx_model_example/#using-an-onnx-model","title":"Using an ONNX model\u00b6","text":""},{"location":"notebooks/onnx_model_example/#example-point-input","title":"Example point input\u00b6","text":""},{"location":"notebooks/onnx_model_example/#example-mask-input","title":"Example mask input\u00b6","text":""},{"location":"notebooks/onnx_model_example/#example-box-and-point-input","title":"Example box and point input\u00b6","text":""},{"location":"notebooks/predictor_example/","title":"\u4f7f\u7528\u200b\u63d0\u793a\u200b\u751f\u6210\u200b\u63a9\u7801","text":"<p>\u200b\u5206\u5272\u200b\u4e00\u5207\u200b\u6a21\u578b\u200bSegment Anything Model\uff08SAM\uff09\u200b\u6839\u636e\u200b\u6307\u793a\u200b\u6240\u200b\u9700\u200b\u5bf9\u8c61\u200b\u7684\u200b\u63d0\u793a\u200b\u6765\u200b\u9884\u6d4b\u200b\u5bf9\u8c61\u200b\u7684\u200b\u63a9\u7801\u200b\u3002\u200b\u8be5\u200b\u6a21\u578b\u200b\u9996\u5148\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u5d4c\u5165\u200bembedding\uff0c\u200b\u4ece\u800c\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u63d0\u793a\u200b\u4e2d\u200b\u9ad8\u6548\u200b\u5730\u200b\u751f\u6210\u200b\u9ad8\u8d28\u91cf\u200b\u7684\u200b\u63a9\u7801\u200b\u3002</p> <p><code>SamPredictor</code>\u200b\u7c7b\u4e3a\u200b\u4f7f\u7528\u200b\u63d0\u793a\u200b\u6a21\u578b\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u63a5\u53e3\u200b\u3002\u200b\u5b83\u200b\u5141\u8bb8\u200b\u7528\u6237\u200b\u9996\u5148\u200b\u4f7f\u7528\u200b<code>set_image</code>\u200b\u65b9\u6cd5\u200b\u8bbe\u7f6e\u200b\u56fe\u50cf\u200b\uff0c\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u8ba1\u7b97\u6240\u200b\u9700\u200b\u7684\u200b\u56fe\u50cf\u200b\u5d4c\u5165\u200b\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>predict</code>\u200b\u65b9\u6cd5\u200b\u63d0\u4f9b\u200b\u63d0\u793a\u200b\uff0c\u200b\u4ee5\u200b\u4ece\u200b\u8fd9\u4e9b\u200b\u63d0\u793a\u200b\u4e2d\u200b\u9ad8\u6548\u200b\u5730\u200b\u9884\u6d4b\u200b\u63a9\u7801\u200b\u3002\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u63a5\u53d7\u200b\u70b9\u200b\u63d0\u793a\u200b\u3001\u200b\u6846\u200b\u63d0\u793a\u200b\u4ee5\u53ca\u200b\u6765\u81ea\u200b\u524d\u200b\u4e00\u4e2a\u200b\u9884\u6d4b\u200b\u8fed\u4ee3\u200b\u7684\u200b\u63a9\u7801\u200b\u4f5c\u4e3a\u200b\u8f93\u5165\u200b\u3002</p> In\u00a0[2]: Copied! <pre>from IPython.display import display, HTML\ndisplay(HTML(\n\"\"\"\n&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/EanYang7/segment-anything/blob/main/docs/notebooks/predictor_example.ipynb\"&gt;\n  &lt;img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"\u200b\u4f7f\u7528\u200bColab\u200b\u6253\u5f00\u200b\"/&gt;\n&lt;/a&gt;\n\"\"\"\n))\n</pre> from IPython.display import display, HTML display(HTML( \"\"\"   \"\"\" )) <p>\u200b\u5982\u679c\u200b\u5728\u200b\u672c\u5730\u200b\u4f7f\u7528\u200bJupyter\u200b\u8fd0\u884c\u200b\uff0c\u200b\u8bf7\u200b\u9996\u5148\u200b\u6309\u7167\u200b\u5b58\u50a8\u200b\u5e93\u4e2d\u200b\u7684\u200b\u5b89\u88c5\u200b\u8bf4\u660e\u200b\u5728\u200b\u60a8\u200b\u7684\u200b\u73af\u5883\u200b\u4e2d\u200b\u5b89\u88c5\u200b<code>segment_anything</code>\u3002\u200b\u5982\u679c\u200b\u5728\u200bGoogle Colab\u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff0c\u200b\u8bf7\u200b\u5c06\u200b\u4e0b\u9762\u200b\u7684\u200b<code>using_colab=True</code>\u200b\u8bbe\u7f6e\u200b\u4e3a\u200bTrue\uff0c\u200b\u5e76\u200b\u8fd0\u884c\u200b\u5355\u5143\u683c\u200b\u3002\u200b\u5728\u200bColab\u200b\u4e2d\u200b\uff0c\u200b\u8bf7\u200b\u786e\u4fdd\u200b\u5728\u200b\u201c\u200b\u7f16\u8f91\u200b\u201d-&gt;\u201c\u200b\u7b14\u8bb0\u672c\u200b\u8bbe\u7f6e\u200b\u201d-&gt;\u201c\u200b\u786c\u4ef6\u200b\u52a0\u901f\u5668\u200b\u201d\u200b\u4e0b\u200b\u9009\u62e9\u200b\u201cGPU\u201d\u3002</p> In\u00a0[3]: Copied! <pre>using_colab = False\n</pre> using_colab = False In\u00a0[4]: Copied! <pre>if using_colab:\n    import torch\n    import torchvision\n    print(\"PyTorch version:\", torch.__version__)\n    print(\"Torchvision version:\", torchvision.__version__)\n    print(\"CUDA is available:\", torch.cuda.is_available())\n    import sys\n    !{sys.executable} -m pip install opencv-python matplotlib\n    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n    \n    !mkdir images\n    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n        \n    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n</pre> if using_colab:     import torch     import torchvision     print(\"PyTorch version:\", torch.__version__)     print(\"Torchvision version:\", torchvision.__version__)     print(\"CUDA is available:\", torch.cuda.is_available())     import sys     !{sys.executable} -m pip install opencv-python matplotlib     !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'          !mkdir images     !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg     !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg              !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth <p>\u200b\u70b9\u200b\u3001\u200b\u6846\u200b\u548c\u200b\u63a9\u7801\u200b \u200b\u6240\u200b\u9700\u200b\u7684\u200b\u5bfc\u5165\u200b\u548c\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[5]: Copied! <pre>import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2\n</pre> import numpy as np import torch import matplotlib.pyplot as plt import cv2 In\u00a0[6]: Copied! <pre>def show_mask(mask, ax, random_color=False):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    ax.imshow(mask_image)\n    \ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n    \ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n</pre> def show_mask(mask, ax, random_color=False):     if random_color:         color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)     else:         color = np.array([30/255, 144/255, 255/255, 0.6])     h, w = mask.shape[-2:]     mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)     ax.imshow(mask_image)      def show_points(coords, labels, ax, marker_size=375):     pos_points = coords[labels==1]     neg_points = coords[labels==0]     ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)     ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)         def show_box(box, ax):     x0, y0 = box[0], box[1]     w, h = box[2] - box[0], box[3] - box[1]     ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))      In\u00a0[7]: Copied! <pre>image = cv2.imread('images/truck.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n</pre> image = cv2.imread('images/truck.jpg') image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) In\u00a0[8]: Copied! <pre>plt.figure(figsize=(10,10))\nplt.imshow(image)\nplt.axis('on')\nplt.show()\n</pre> plt.figure(figsize=(10,10)) plt.imshow(image) plt.axis('on') plt.show() <p>\u200b\u52a0\u8f7d\u200bSAM\u200b\u6a21\u578b\u200b\u548c\u200b\u9884\u6d4b\u5668\u200b\u3002\u200b\u8bf7\u200b\u5c06\u200b\u4e0b\u9762\u200b\u7684\u200b\u8def\u5f84\u200b\u66f4\u200b\u6539\u4e3a\u200b\u6307\u5411\u200bSAM\u200b\u68c0\u67e5\u70b9\u200b\u7684\u200b\u8def\u5f84\u200b\u3002\u200b\u5efa\u8bae\u200b\u5728\u200bCUDA\u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u9ed8\u8ba4\u200b\u6a21\u578b\u200b\u4ee5\u200b\u83b7\u5f97\u6700\u4f73\u200b\u7ed3\u679c\u200b\u3002</p> In\u00a0[13]: Copied! <pre>import sys\n# sys.path.append(\"..\")\nfrom segment_anything import sam_model_registry, SamPredictor\n\nsam_checkpoint = \"../../checkpoints/sam_vit_h_4b8939.pth\"\nmodel_type = \"vit_h\"\n\ndevice = \"cuda\"\n\nsam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\nsam.to(device=device)\n\npredictor = SamPredictor(sam)\n</pre> import sys # sys.path.append(\"..\") from segment_anything import sam_model_registry, SamPredictor  sam_checkpoint = \"../../checkpoints/sam_vit_h_4b8939.pth\" model_type = \"vit_h\"  device = \"cuda\"  sam = sam_model_registry[model_type](checkpoint=sam_checkpoint) sam.to(device=device)  predictor = SamPredictor(sam) <p>\u200b\u901a\u8fc7\u200b\u8c03\u7528\u200b <code>SamPredictor.set_image</code> \u200b\u5904\u7406\u200b\u56fe\u50cf\u200b\u4ee5\u200b\u751f\u6210\u200b\u56fe\u50cf\u200b\u5d4c\u5165\u200b\u3002<code>SamPredictor</code> \u200b\u4f1a\u200b\u8bb0\u4f4f\u200b\u8fd9\u4e2a\u200b\u5d4c\u5165\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5728\u200b\u540e\u7eed\u200b\u7684\u200b\u63a9\u7801\u200b\u9884\u6d4b\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002</p> In\u00a0[14]: Copied! <pre>predictor.set_image(image)\n</pre> predictor.set_image(image) <p>\u200b\u8981\u200b\u9009\u62e9\u200b\u5361\u8f66\u200b\uff0c\u200b\u8bf7\u200b\u9009\u62e9\u200b\u5b83\u200b\u4e0a\u9762\u200b\u7684\u200b\u4e00\u4e2a\u70b9\u200b\u3002\u200b\u70b9\u4ee5\u200b (x, y) \u200b\u683c\u5f0f\u200b\u8f93\u5165\u200b\u5230\u200b\u6a21\u578b\u200b\u4e2d\u200b\uff0c\u200b\u5e76\u200b\u5e26\u6709\u200b\u6807\u7b7e\u200b 1\uff08\u200b\u524d\u200b\u666f\u70b9\u200b\uff09\u200b\u6216\u200b 0\uff08\u200b\u80cc\u666f\u200b\u70b9\u200b\uff09\u3002\u200b\u53ef\u4ee5\u200b\u8f93\u5165\u200b\u591a\u4e2a\u200b\u70b9\u200b\uff1b\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u53ea\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u70b9\u200b\u3002\u200b\u6240\u9009\u200b\u7684\u200b\u70b9\u5c06\u200b\u663e\u793a\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u7684\u200b\u661f\u53f7\u200b\u3002</p> In\u00a0[15]: Copied! <pre>input_point = np.array([[500, 375]])\ninput_label = np.array([1])\n</pre> input_point = np.array([[500, 375]]) input_label = np.array([1]) In\u00a0[16]: Copied! <pre>plt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.axis('on')\nplt.show()\n</pre> plt.figure(figsize=(10,10)) plt.imshow(image) show_points(input_point, input_label, plt.gca()) plt.axis('on') plt.show()   <p>\u200b\u4f7f\u7528\u200b <code>SamPredictor.predict</code> \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002\u200b\u6a21\u578b\u200b\u8fd4\u56de\u200b\u63a9\u7801\u200b\u3001\u200b\u8fd9\u4e9b\u200b\u63a9\u7801\u200b\u7684\u200b\u8d28\u91cf\u200b\u9884\u6d4b\u200b\u4ee5\u53ca\u200b\u4f4e\u5206\u8fa8\u7387\u200b\u63a9\u7801\u200b\u903b\u8f91\u200blogits\uff0c\u200b\u53ef\u4ee5\u200b\u4f20\u9012\u200b\u7ed9\u200b\u4e0b\u200b\u4e00\u8f6e\u200b\u7684\u200b\u9884\u6d4b\u200b\u3002</p> In\u00a0[18]: Copied! <pre>masks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,\n)\n</pre> masks, scores, logits = predictor.predict(     point_coords=input_point,     point_labels=input_label,     multimask_output=True, ) <p>\u200b\u5f53\u200b <code>multimask_output=True</code>\uff08\u200b\u9ed8\u8ba4\u8bbe\u7f6e\u200b\uff09\u200b\u65f6\u200b\uff0cSAM\u200b\u4f1a\u200b\u8f93\u51fa\u200b3\u200b\u4e2a\u200b\u63a9\u7801\u200b\uff0c\u200b\u5176\u4e2d\u200b <code>scores</code> \u200b\u7ed9\u51fa\u200b\u4e86\u200b\u6a21\u578b\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u63a9\u7801\u200b\u8d28\u91cf\u200b\u7684\u200b\u81ea\u6211\u200b\u8bc4\u4f30\u200b\u3002\u200b\u6b64\u200b\u8bbe\u7f6e\u200b\u9002\u7528\u200b\u4e8e\u200b\u6a21\u68f1\u4e24\u53ef\u200b\u7684\u200b\u8f93\u5165\u200b\u63d0\u793a\u200b\uff0c\u200b\u6709\u52a9\u4e8e\u200b\u6a21\u578b\u200b\u533a\u5206\u200b\u4e0e\u200b\u63d0\u793a\u200b\u4e00\u81f4\u200b\u7684\u200b\u4e0d\u540c\u200b\u5bf9\u8c61\u200b\u3002\u200b\u5f53\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>False</code> \u200b\u65f6\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u63a9\u7801\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u6a21\u68f1\u4e24\u53ef\u200b\u7684\u200b\u63d0\u793a\u200b\uff0c\u200b\u6bd4\u5982\u200b\u4e00\u4e2a\u200b\u5355\u200b\u4e00\u70b9\u200b\uff0c\u200b\u5efa\u8bae\u200b\u5373\u4f7f\u200b\u53ea\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u63a9\u7801\u200b\u4e5f\u200b\u4f7f\u7528\u200b <code>multimask_output=True</code>\uff1b\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u9009\u62e9\u200b\u5728\u200b <code>scores</code> \u200b\u4e2d\u200b\u8fd4\u56de\u200b\u7684\u200b\u5206\u6570\u200b\u6700\u9ad8\u200b\u7684\u200b\u63a9\u7801\u200b\u6765\u200b\u9009\u62e9\u200b\u6700\u4f73\u200b\u7684\u200b\u5355\u4e00\u200b\u63a9\u7801\u200b\u3002\u200b\u8fd9\u200b\u901a\u5e38\u200b\u4f1a\u200b\u4ea7\u751f\u200b\u66f4\u597d\u200b\u7684\u200b\u63a9\u7801\u200b\u3002</p> In\u00a0[19]: Copied! <pre>masks.shape  # (number_of_masks) x H x W\n</pre> masks.shape  # (number_of_masks) x H x W Out[19]: <pre>(3, 1200, 1800)</pre> In\u00a0[20]: Copied! <pre>for i, (mask, score) in enumerate(zip(masks, scores)):\n    plt.figure(figsize=(10,10))\n    plt.imshow(image)\n    show_mask(mask, plt.gca())\n    show_points(input_point, input_label, plt.gca())\n    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n    plt.axis('off')\n    plt.show()\n</pre> for i, (mask, score) in enumerate(zip(masks, scores)):     plt.figure(figsize=(10,10))     plt.imshow(image)     show_mask(mask, plt.gca())     show_points(input_point, input_label, plt.gca())     plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)     plt.axis('off')     plt.show()      <p>The single input point is ambiguous, and the model has returned multiple objects consistent with it. To obtain a single object, multiple points can be provided. If available, a mask from a previous iteration can also be supplied to the model to aid in prediction. When specifying a single object with multiple prompts, a single mask can be requested by setting <code>multimask_output=False</code>.</p> In\u00a0[13]: Copied! <pre>input_point = np.array([[500, 375], [1125, 625]])\ninput_label = np.array([1, 1])\n\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n</pre> input_point = np.array([[500, 375], [1125, 625]]) input_label = np.array([1, 1])  mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask In\u00a0[14]: Copied! <pre>masks, _, _ = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    mask_input=mask_input[None, :, :],\n    multimask_output=False,\n)\n</pre> masks, _, _ = predictor.predict(     point_coords=input_point,     point_labels=input_label,     mask_input=mask_input[None, :, :],     multimask_output=False, ) In\u00a0[15]: Copied! <pre>masks.shape\n</pre> masks.shape Out[15]: <pre>(1, 1200, 1800)</pre> In\u00a0[16]: Copied! <pre>plt.figure(figsize=(10,10))\nplt.imshow(image)\nshow_mask(masks, plt.gca())\nshow_points(input_point, input_label, plt.gca())\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(10,10)) plt.imshow(image) show_mask(masks, plt.gca()) show_points(input_point, input_label, plt.gca()) plt.axis('off') plt.show()  <p>To exclude the car and specify just the window, a background point (with label 0, here shown in red) can be supplied.</p> In\u00a0[17]: Copied! <pre>input_point = np.array([[500, 375], [1125, 625]])\ninput_label = np.array([1, 0])\n\nmask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n</pre> input_point = np.array([[500, 375], [1125, 625]]) input_label = np.array([1, 0])  mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask In\u00a0[18]: Copied! <pre>masks, _, _ = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    mask_input=mask_input[None, :, :],\n    multimask_output=False,\n)\n</pre> masks, _, _ = predictor.predict(     point_coords=input_point,     point_labels=input_label,     mask_input=mask_input[None, :, :],     multimask_output=False, ) In\u00a0[19]: Copied! <pre>plt.figure(figsize=(10, 10))\nplt.imshow(image)\nshow_mask(masks, plt.gca())\nshow_points(input_point, input_label, plt.gca())\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(10, 10)) plt.imshow(image) show_mask(masks, plt.gca()) show_points(input_point, input_label, plt.gca()) plt.axis('off') plt.show()  <p>The model can also take a box as input, provided in xyxy format.</p> In\u00a0[20]: Copied! <pre>input_box = np.array([425, 600, 700, 875])\n</pre> input_box = np.array([425, 600, 700, 875]) In\u00a0[21]: Copied! <pre>masks, _, _ = predictor.predict(\n    point_coords=None,\n    point_labels=None,\n    box=input_box[None, :],\n    multimask_output=False,\n)\n</pre> masks, _, _ = predictor.predict(     point_coords=None,     point_labels=None,     box=input_box[None, :],     multimask_output=False, ) In\u00a0[22]: Copied! <pre>plt.figure(figsize=(10, 10))\nplt.imshow(image)\nshow_mask(masks[0], plt.gca())\nshow_box(input_box, plt.gca())\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(10, 10)) plt.imshow(image) show_mask(masks[0], plt.gca()) show_box(input_box, plt.gca()) plt.axis('off') plt.show() <p>Points and boxes may be combined, just by including both types of prompts to the predictor. Here this can be used to select just the trucks's tire, instead of the entire wheel.</p> In\u00a0[23]: Copied! <pre>input_box = np.array([425, 600, 700, 875])\ninput_point = np.array([[575, 750]])\ninput_label = np.array([0])\n</pre> input_box = np.array([425, 600, 700, 875]) input_point = np.array([[575, 750]]) input_label = np.array([0]) In\u00a0[24]: Copied! <pre>masks, _, _ = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    box=input_box,\n    multimask_output=False,\n)\n</pre> masks, _, _ = predictor.predict(     point_coords=input_point,     point_labels=input_label,     box=input_box,     multimask_output=False, ) In\u00a0[25]: Copied! <pre>plt.figure(figsize=(10, 10))\nplt.imshow(image)\nshow_mask(masks[0], plt.gca())\nshow_box(input_box, plt.gca())\nshow_points(input_point, input_label, plt.gca())\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(10, 10)) plt.imshow(image) show_mask(masks[0], plt.gca()) show_box(input_box, plt.gca()) show_points(input_point, input_label, plt.gca()) plt.axis('off') plt.show() <p>SamPredictor can take multiple input prompts for the same image, using <code>predict_torch</code> method. This method assumes input points are already torch tensors and have already been transformed to the input frame. For example, imagine we have several box outputs from an object detector.</p> In\u00a0[26]: Copied! <pre>input_boxes = torch.tensor([\n    [75, 275, 1725, 850],\n    [425, 600, 700, 875],\n    [1375, 550, 1650, 800],\n    [1240, 675, 1400, 750],\n], device=predictor.device)\n</pre> input_boxes = torch.tensor([     [75, 275, 1725, 850],     [425, 600, 700, 875],     [1375, 550, 1650, 800],     [1240, 675, 1400, 750], ], device=predictor.device) <p>Transform the boxes to the input frame, then predict masks. <code>SamPredictor</code> stores the necessary transform as the <code>transform</code> field for easy access, though it can also be instantiated directly for use in e.g. a dataloader (see <code>segment_anything.utils.transforms</code>).</p> In\u00a0[27]: Copied! <pre>transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2])\nmasks, _, _ = predictor.predict_torch(\n    point_coords=None,\n    point_labels=None,\n    boxes=transformed_boxes,\n    multimask_output=False,\n)\n</pre> transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2]) masks, _, _ = predictor.predict_torch(     point_coords=None,     point_labels=None,     boxes=transformed_boxes,     multimask_output=False, ) In\u00a0[28]: Copied! <pre>masks.shape  # (batch_size) x (num_predicted_masks_per_input) x H x W\n</pre> masks.shape  # (batch_size) x (num_predicted_masks_per_input) x H x W Out[28]: <pre>torch.Size([4, 1, 1200, 1800])</pre> In\u00a0[29]: Copied! <pre>plt.figure(figsize=(10, 10))\nplt.imshow(image)\nfor mask in masks:\n    show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\nfor box in input_boxes:\n    show_box(box.cpu().numpy(), plt.gca())\nplt.axis('off')\nplt.show()\n</pre> plt.figure(figsize=(10, 10)) plt.imshow(image) for mask in masks:     show_mask(mask.cpu().numpy(), plt.gca(), random_color=True) for box in input_boxes:     show_box(box.cpu().numpy(), plt.gca()) plt.axis('off') plt.show() <p>If all prompts are available in advance, it is possible to run SAM directly in an end-to-end fashion. This also allows batching over images.</p> In\u00a0[30]: Copied! <pre>image1 = image  # truck.jpg from above\nimage1_boxes = torch.tensor([\n    [75, 275, 1725, 850],\n    [425, 600, 700, 875],\n    [1375, 550, 1650, 800],\n    [1240, 675, 1400, 750],\n], device=sam.device)\n\nimage2 = cv2.imread('images/groceries.jpg')\nimage2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\nimage2_boxes = torch.tensor([\n    [450, 170, 520, 350],\n    [350, 190, 450, 350],\n    [500, 170, 580, 350],\n    [580, 170, 640, 350],\n], device=sam.device)\n</pre> image1 = image  # truck.jpg from above image1_boxes = torch.tensor([     [75, 275, 1725, 850],     [425, 600, 700, 875],     [1375, 550, 1650, 800],     [1240, 675, 1400, 750], ], device=sam.device)  image2 = cv2.imread('images/groceries.jpg') image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB) image2_boxes = torch.tensor([     [450, 170, 520, 350],     [350, 190, 450, 350],     [500, 170, 580, 350],     [580, 170, 640, 350], ], device=sam.device) <p>Both images and prompts are input as PyTorch tensors that are already transformed to the correct frame. Inputs are packaged as a list over images, which each element is a dict that takes the following keys:</p> <ul> <li><code>image</code>: The input image as a PyTorch tensor in CHW format.</li> <li><code>original_size</code>: The size of the image before transforming for input to SAM, in (H, W) format.</li> <li><code>point_coords</code>: Batched coordinates of point prompts.</li> <li><code>point_labels</code>: Batched labels of point prompts.</li> <li><code>boxes</code>: Batched input boxes.</li> <li><code>mask_inputs</code>: Batched input masks.</li> </ul> <p>If a prompt is not present, the key can be excluded.</p> In\u00a0[31]: Copied! <pre>from segment_anything.utils.transforms import ResizeLongestSide\nresize_transform = ResizeLongestSide(sam.image_encoder.img_size)\n\ndef prepare_image(image, transform, device):\n    image = transform.apply_image(image)\n    image = torch.as_tensor(image, device=device.device) \n    return image.permute(2, 0, 1).contiguous()\n</pre> from segment_anything.utils.transforms import ResizeLongestSide resize_transform = ResizeLongestSide(sam.image_encoder.img_size)  def prepare_image(image, transform, device):     image = transform.apply_image(image)     image = torch.as_tensor(image, device=device.device)      return image.permute(2, 0, 1).contiguous() In\u00a0[32]: Copied! <pre>batched_input = [\n     {\n         'image': prepare_image(image1, resize_transform, sam),\n         'boxes': resize_transform.apply_boxes_torch(image1_boxes, image1.shape[:2]),\n         'original_size': image1.shape[:2]\n     },\n     {\n         'image': prepare_image(image2, resize_transform, sam),\n         'boxes': resize_transform.apply_boxes_torch(image2_boxes, image2.shape[:2]),\n         'original_size': image2.shape[:2]\n     }\n]\n</pre> batched_input = [      {          'image': prepare_image(image1, resize_transform, sam),          'boxes': resize_transform.apply_boxes_torch(image1_boxes, image1.shape[:2]),          'original_size': image1.shape[:2]      },      {          'image': prepare_image(image2, resize_transform, sam),          'boxes': resize_transform.apply_boxes_torch(image2_boxes, image2.shape[:2]),          'original_size': image2.shape[:2]      } ] <p>Run the model.</p> In\u00a0[33]: Copied! <pre>batched_output = sam(batched_input, multimask_output=False)\n</pre> batched_output = sam(batched_input, multimask_output=False) <p>The output is a list over results for each input image, where list elements are dictionaries with the following keys:</p> <ul> <li><code>masks</code>: A batched torch tensor of predicted binary masks, the size of the original image.</li> <li><code>iou_predictions</code>: The model's prediction of the quality for each mask.</li> <li><code>low_res_logits</code>: Low res logits for each mask, which can be passed back to the model as mask input on a later iteration.</li> </ul> In\u00a0[34]: Copied! <pre>batched_output[0].keys()\n</pre> batched_output[0].keys() Out[34]: <pre>dict_keys(['masks', 'iou_predictions', 'low_res_logits'])</pre> In\u00a0[35]: Copied! <pre>fig, ax = plt.subplots(1, 2, figsize=(20, 20))\n\nax[0].imshow(image1)\nfor mask in batched_output[0]['masks']:\n    show_mask(mask.cpu().numpy(), ax[0], random_color=True)\nfor box in image1_boxes:\n    show_box(box.cpu().numpy(), ax[0])\nax[0].axis('off')\n\nax[1].imshow(image2)\nfor mask in batched_output[1]['masks']:\n    show_mask(mask.cpu().numpy(), ax[1], random_color=True)\nfor box in image2_boxes:\n    show_box(box.cpu().numpy(), ax[1])\nax[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> fig, ax = plt.subplots(1, 2, figsize=(20, 20))  ax[0].imshow(image1) for mask in batched_output[0]['masks']:     show_mask(mask.cpu().numpy(), ax[0], random_color=True) for box in image1_boxes:     show_box(box.cpu().numpy(), ax[0]) ax[0].axis('off')  ax[1].imshow(image2) for mask in batched_output[1]['masks']:     show_mask(mask.cpu().numpy(), ax[1], random_color=True) for box in image2_boxes:     show_box(box.cpu().numpy(), ax[1]) ax[1].axis('off')  plt.tight_layout() plt.show()"},{"location":"notebooks/predictor_example/#sam","title":"SAM\u200b\u4f7f\u7528\u200b\u63d0\u793a\u200b\u751f\u6210\u200b\u5bf9\u8c61\u200b\u63a9\u7801\u200b\u00b6","text":""},{"location":"notebooks/predictor_example/","title":"\u8bbe\u7f6e\u200b\u73af\u5883\u200b\u00b6","text":""},{"location":"notebooks/predictor_example/","title":"\u8bbe\u7f6e\u200b\u00b6","text":""},{"location":"notebooks/predictor_example/","title":"\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u00b6","text":""},{"location":"notebooks/predictor_example/#sam","title":"\u4f7f\u7528\u200bSAM\u200b\u9009\u62e9\u5bf9\u8c61\u200b\u00b6","text":""},{"location":"notebooks/predictor_example/#specifying-a-specific-object-with-additional-points","title":"Specifying a specific object with additional points\u00b6","text":""},{"location":"notebooks/predictor_example/#specifying-a-specific-object-with-a-box","title":"Specifying a specific object with a box\u00b6","text":""},{"location":"notebooks/predictor_example/#combining-points-and-boxes","title":"Combining points and boxes\u00b6","text":""},{"location":"notebooks/predictor_example/#batched-prompt-inputs","title":"Batched prompt inputs\u00b6","text":""},{"location":"notebooks/predictor_example/#end-to-end-batched-inference","title":"End-to-end batched inference\u00b6","text":""}]}